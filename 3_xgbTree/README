################################################################################
# xgbTree using Caret to build a model predicting Tm and pHt of i-motifs
################################################################################
#-------------------------------------------------------------------------------
# APPROACH
#-------------------------------------------------------------------------------
A model was built using all data (196 sequences) as training set. Without the
affix shuff means that the 196 sequences we're not shuffled once initially.
Using the optimal hyperparameter obtained from this method,

Optimal hyperparameters:
tuneGrid = expand.grid(nrounds=c(1000),
                       eta=c(0.01),
                       max_depth=c(4),
                       min_child_weight=c(5),
                       subsample=c(0.6),
                       gamma=c(0), 
                       colsample_bytree=c(1))
					   
The model was (re)trained using 80% of data chosen randomly (seed=825). The
resulting model was tested on the test set (20%). In the observed vs. predicted
plot, the metric was calculated on the test set only. 		

Models are named as model_<ind>, where ind identifies the set of tuning parameters 
(see below). 

If you check the rest/ in Tm/, there are some affixes used. 'shuff' affix means that 
the data was shuffled once before using for training. noscaling affix means that the
predictor values were not centred and scaled. 'replicate' affix is exactly what it 
means, this was done to ensure reproducibility.

model_2 tuning parameters were finally used for the tuning. noLength means that 
total sequence length is not included as feature. This was done because notably, 
length did not appear in most models generated by Eureqa (iMotif_chosen_BE in the
case of Tm prediction, adopted also for pHt prediction).

2. Own scaling of importance score
Caret's varImp is used to obtain importance score and it intrinsically matches
the procedure based on the ML method. 

varImp() for boosted trees in Caret is a wrapper around the function,
relative.influence() from the gbm package. If scale=TRUE in varIMP(), Caret applies 
a scaling method such that values would be from 0 to 100. This means that the 
feature with the highest and least raw importance scores have scaled scores
100 and 0, respectively. 
To avoid misleading 0s in this case, the raw importance scores from 
varImp(scale=FALSE) were simply scaled to the feature with the highest raw score
by doing score/max(score). 

Example:
Feature Raw   Scaled-caret                Scale-own
Feat1   0.814 100                         0.814/0.814*100=100 
Feat2   0.045 (0.045-0.017)*125.4705=3.51 0.045/0.814*100=5.53
Feat3   0.038 (0.038-0.017)*125.4705=2.63 4.67
Feat4   0.017 0                           2.09

Caret
- scaling factor: 100/(max-min), in this case its 125.4705
- to calculate scaled score, (x-min)*scaling factor

#-------------------------------------------------------------------------------
# ANALYSES
#-------------------------------------------------------------------------------
1. Tm
Using model_2 parameters, all data as training data set, with and without total 
length as features yielded the same optimal set of hyperparameters:

  nrounds max_depth  eta gamma colsample_bytree min_child_weight subsample
37    1000         4 0.01     0                1                5       0.6

Ranking of feature importance:
All: C-length-T3-T1-T2
No length: C-T3-T2-T1

Both models (with and without length) were then further tuned using 80% of 
training data and then validated with the 20%. 

Performance on 80% training data:

with total length:
nrounds  eta max_depth min_child_weight subsample gamma colsample_bytree
1    1000 0.01         4                5       0.6     0                1
    RMSE  Rsquared      MAE    RMSESD  RsquaredSD     MAESD
1 1.4011 0.9758311 1.135564 0.1692699 0.005186407 0.1139668

without total length: 
nrounds  eta max_depth min_child_weight subsample gamma colsample_bytree
1    1000 0.01         4                5       0.6     0                1
      RMSE  Rsquared      MAE    RMSESD  RsquaredSD     MAESD
1 1.376196 0.9767373 1.120195 0.1639031 0.005370633 0.1081146

The model with the total length as one of the feature showed a slightly better 
performance, r^2=0.9811 (vs. r^2=0.9803). Performance measures based on the test 
data (20%). 

2. pHt

Using model_2 parameters, all data as training data set, with and without total length
as features yielded different optimal set of hyperparameters:

with total length:
      nrounds max_depth  eta gamma colsample_bytree min_child_weight subsample
176     750         8 0.01     0                1                3       0.6

without total length:
      nrounds max_depth  eta gamma colsample_bytree min_child_weight subsample
150    1500         6 0.01     0                1               10       0.6

As in Tm's case, both models (with and without length) were then further tuned 
using 80% of training data and then validated with the 20% (same division of
dataset as in Tm). 

Ranking of feature importance:
All: C-length-T1-T3-T2
No length: C-T2-T1-T3

Performance on 80% training data:

with total length:
    nrounds max_depth  eta min_child_weight subsample gamma colsample_bytree
1     750         8 0.01                3       0.6     0                1
        RMSE  Rsquared        MAE      RMSESD RsquaredSD       MAESD
1 0.05632807 0.9424578 0.04353706 0.008222115  0.0180082 0.007648299

without total length:
    nrounds max_depth  eta min_child_weight subsample gamma colsample_bytree
1    1500         6 0.01               10       0.6     0                1
        RMSE  Rsquared        MAE      RMSESD RsquaredSD       MAESD
1 0.05399737 0.9464987 0.04250962 0.007408617 0.01318266 0.005437334

Model without the total length feature performed slightly better r^2=0.9399 
(vs. r^2=0.9477).

#-------------------------------------------------------------------------------
# Feature importance (80% training)
#-------------------------------------------------------------------------------
1. Tm

All: C-T3-T2-T1
No length: C-length-T3-T1-T2

2. pHt

All: C-length-T1-T3-T2
No length: C-T2-T1-T3

################################################################################
# Tuning Parameters
################################################################################

# Reference: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/

general
#-------------------
seed = 825
cpu = 2 (if not explicity stated)
CenterScaling = TRUE

FIT <- train(x=trainData,
             y=targetData,
             weights = NULL,
             method = "xgbTree",
             metric = "RMSE",
             trControl=trainControl(method = "repeatedcv",
                                    number = CVnum,
                                    repeats = CVrep,
                                    seeds = seeds),
             tuneGrid=tuneGrid,
             verbose=TRUE
)
#-------------------

model_1
#-------------------
features.v = combinations
target = "Tm"
trainDataFr = c(0.5, 0.6, 0.7, 0.8, 0.9)

ind=1
CVnum  = 2
CVrep  = 2
tuneGrid = expand.grid(nrounds=c(500, 750, 1000, 1500, 2000, 2500, 3000, 3500),
                       max_depth=c(6),
                       eta=c(0.1),
                       min_child_weight=c(5),
                       subsample=c(1),
                       gamma=c(0),
                       colsample_bytree = c(1))
#-------------------

model_2
#-------------------
features.v = c("length", "C", "T1", "T2", "T3")
target = "Tm"
trainDataFr = 1

ind = 2
# K-fold validation
CVnum   = 5
# Repeat of K=fold validation
CVrep   = 3
tuneGrid = expand.grid(nrounds=c(10, 50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500),
                       # Same as in GBM, typically [3,10]
                       max_depth=c(4, 6, 8),
                       # Learning rate
                       eta=c(0.1, 0.01),
                       # Minimum number of values assigned to a leaf
                       min_child_weight=c(3, 5, 10),
                       # Same as the subsample of GBM. Denotes the fraction of 
                       # observations to be randomly sampled for each tree, typically [0.5,1]
                       # To avoid trees becoming highly correlated
                       subsample=c(1, 0.6),
                       # A node is split only when the resulting split gives a positive 
                       # reduction in the loss function. Gamma specifies the minimum loss 
                       # reduction required to make a split.
                       # gamma=0 means No regularisation, [0,infinity]
                       gamma=c(0),
                       # Similar to max_features in GBM. Denotes the fraction of columns/features 
                       # to be randomly sampled for each tree.
                       # colsample_bytree=1 means no subsampling
                       colsample_bytree = c(1))
#-------------------

model_3
#-------------------
cpu      = 3
ind      = 3
# K-fold validation
CVnum    = 5
# Repeat of K=fold validation
CVrep    = 3
tuneGrid = expand.grid(nrounds=c(10, 50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500),
                       # Same as in GBM, typically [3,10]
                       max_depth=c(3, 4, 6, 8),
                       # Learning rate
                       eta=c(0.1, 0.01),
                       # Minimum number of values assigned to a leaf
                       min_child_weight=c(3, 5, 10),
                       # Same as the subsample of GBM. Denotes the fraction of 
                       # observations to be randomly sampled for each tree, typically [0.5,1]
                       # To avoid trees becoming highly correlated
                       subsample=c(1, 0.6, 0.5),
                       # A node is split only when the resulting split gives a positive 
                       # reduction in the loss function. Gamma specifies the minimum loss 
                       # reduction required to make a split.
                       # gamma=0 means No regularisation, [0,infinity]
                       gamma=c(0),
                       # Similar to max_features in GBM. Denotes the fraction of columns/features 
                       # to be randomly sampled for each tree.
                       # colsample_bytree=1 means no subsampling
                       colsample_bytree = c(1))
#-------------------

################################################################################
LOG

18/12/2019 - restructured and added Analyses part
25/11/2019 - Written
